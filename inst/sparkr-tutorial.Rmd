---
title: "SparkR"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

## Objectifs

Cette formation va reprendre les exemples des tutoriaux pyspark sur parquet et hivemetastore.
Il est fortement recommandé de parcourir les formations précedentes en python (pyspark). Les concepts liés à Spark sont indépendant du langage pour "appeler" Spark. 

L'objectif est donc de voir comment avoir recours à Spark depuis R grace à l'API SparkR. 
Une librairie d'accès concurrente existe "Sparklyr"

Je prefère présenter la lirairie d'accès proposer par le projet Apache Spark.

## Initialisation du contexte Spark

Lors d'une phase de developpement sur donnée volumineuse, il est interessant d'utiliser une configuration dynamique du nombre d'executeurs afin de relacher les ressources lorsque l'on execute pas de taches.

```{r build_context, exercise=TRUE}
library(SparkR)
# Initialize SparkSession
sparkR.session(master = "k8s://https://kubernetes.default.svc:443",
               appName = "SparkR",
               sparkHome = Sys.getenv("SPARK_HOME"),
               list(spark.kubernetes.container.image=Sys.getenv("IMAGE_NAME"), 
                    spark.kubernetes.authenticate.driver.serviceAccountName=Sys.getenv("KUBERNETES_SERVICE_ACCOUNT"),
                    spark.kubernetes.driver.pod.name=Sys.getenv("KUBERNETES_POD_NAME"),
                    # config pour allocation dynamique
                    spark.dynamicAllocation.enabled="true",
                    spark.dynamicAllocation.initialExecutors="1",
                    spark.dynamicAllocation.minExecutors="1",
                    spark.dynamicAllocation.maxExecutors="10",
                    spark.dynamicAllocation.executorAllocationRatio="1",
                    spark.dynamicAllocation.shuffleTracking.enabled="true"
                    )
)
```

On peut vérifier la présence de notre executeur spark R

## accès à un fichier csv

Attention la lecture d'un fichier avec inférence du schéma est couteuse car l'ensemble du fichier va etre lu pour en déduire le schéma.

```{r load_sirene, exercise=TRUE}
df <- read.df("s3a://projet-spark-lab/diffusion/formation/data/sirene/sirene.csv", "csv", header = "true", inferSchema = "true")
```

```{r head_sirene, exercise=TRUE}
head(df)
```
## manipulation du dataframe avec l'api SparkR

```{r count_sirene, exercise=TRUE}
ape_count <- summarize(groupBy(df, df$activitePrincipaleEtablissement), count = n(df$activitePrincipaleEtablissement))
head(arrange(ape_count, desc(ape_count$count)))
```
## accès à un fichier via une requête SQL

Ici on va enregistrer la table en lui donnant un nom (sirene) puis l'utiliser dans une requête sql
```{r sql_sirene, exercise=TRUE}
createOrReplaceTempView(df, "sirene")
ape_count_sql<- sql("SELECT count(*) as tot , activitePrincipaleEtablissement FROM sirene group by activitePrincipaleEtablissement order by tot desc;")
head(ape_count_sql)
```

## accès à un fichier parquet

Le format parquet est idéal pour manipuler des données volumineuses.
Le schéma est présent dans le format, pas besoin d'inférer.
```{r read_parquet, exercise=TRUE}
dfparquet <- read.df("s3a://projet-spark-lab/diffusion/formation/data/sirene.parquet", "parquet")
createOrReplaceTempView(dfparquet, "sireneparquet")
```

```{r count_parquet, exercise=TRUE}
ape_count_parquet<- sql("SELECT count(*) as tot , activitePrincipaleEtablissement FROM sireneparquet group by activitePrincipaleEtablissement order by tot desc;")
head(ape_count_parquet)
```
Comme en python, seules les colonnes nécessaires à l'exploitation de la requête (activitePrincipaleEtablissement) ont été téléchargées depuis le stockage ce qui est beaucoup plus rapide

## Hive metastore

Avant de démarrer vous pouvez démarrer le service hive metastore et si vous avez suivi le tutorial sur ce sujet il référence une table sirene.

L'environnement spark a connaissance dans son fichier de configuration de votre metastore hive. On le vérifie en listant les tables disponibles.
```{r show_tables, exercise=TRUE}
head(sql("show tables;"))
```
J'ai les deux tables temporaires csv et parquet que nous avons référencé manuellement et temporairement à cette session spark. J'ai également une table sirene et deux tables données de caisse ( données simulées ) référence dans mon metastore hive.
Si vous n'avez aucune table je vous propose la requête de création de a table.


```{r create_table, , exercise=TRUE}
sql("CREATE EXTERNAL TABLE sirene2 (siren int,nic int,siret bigint,dateFin string,dateDebut string,etatAdministratifEtablissement string,changementEtatAdministratifEtablissement boolean,enseigne1Etablissement string,enseigne2Etablissement string,enseigne3Etablissement string,changementEnseigneEtablissement boolean,denominationUsuelleEtablissement string,changementDenominationUsuelleEtablissement boolean,activitePrincipaleEtablissement string,nomenclatureActivitePrincipaleEtablissement string,changementActivitePrincipaleEtablissement string,caractereEmployeurEtablissement string,changementCaractereEmployeurEtablissement string ) STORED as parquet LOCATION 's3a://projet-spark-lab/diffusion/formation/data/sirene-10-partitions.parquet'")
```
```{r show_tables2, exercise=TRUE}
head(sql("show tables;"))
```
```{r hiveql, exercise=TRUE}
hivedf<-sql('SELECT count(*) as tot , activitePrincipaleEtablissement FROM sirene2 group by activitePrincipaleEtablissement order by tot desc LIMIT 10')
head(hivedf)
```

```{r kill_session, exercise=TRUE}
sparkR.session.stop()
```
